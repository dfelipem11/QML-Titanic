{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c1688c",
   "metadata": {},
   "source": [
    "# Parcial final: Daniel Meneses Rojas C.C. 1036671105\n",
    "\n",
    "### Modelo QML Titanic Original\n",
    "\n",
    "### Ansatz= **StronglyEntanglingLayers**\n",
    "### Optimizador= **GradientDescentOptimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716bb709",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df668049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math                                                 # Libreria para calculos matemáticos\n",
    "import pandas as pd                                         # Libreria para manipular de datos\n",
    "import pennylane as qml                                     # Libreria necesaria para computación cuántica\n",
    "from pennylane import numpy as np                           # Versión diferenciable de numpy\n",
    "from pennylane.optimize import GradientDescentOptimizer     # Optimizador básico\n",
    "\n",
    "# Libreria para dividir los datos de entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split        \n",
    "# Métricas de evaluación\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7174e1",
   "metadata": {},
   "source": [
    "## **I.** Preprocesamiento de los datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb676de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')                      # Carga el dataset desde un archivo CSV\n",
    "df_train['Pclass'] = df_train['Pclass'].astype(str)      # Convierte la columna 'Pclass' a tipo string\n",
    "\n",
    "# Codificación one-hot para las variables categóricas\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train[['Pclass', 'Sex', 'Embarked']])], axis=1)\n",
    "\n",
    "# Llena los valores faltantes de 'Age' con la mediana\n",
    "df_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\n",
    "\n",
    "# Crea una nueva columna para identificar si es niño (<12 años)\n",
    "df_train['is_child'] = df_train['Age'].map(lambda x: 1 if x < 12 else 0)\n",
    "\n",
    "# Selecciona las variables para el modelo\n",
    "cols_model = ['is_child', 'Pclass_1', 'Pclass_2', 'Sex_female']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dcf4b7",
   "metadata": {},
   "source": [
    "## **II.** División de datos de entrada y datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30114ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de datos de entrenamiento y datos de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[cols_model], df_train['Survived'], test_size=0.10, random_state=42, stratify=df_train['Survived'])  # División estratificada\n",
    "\n",
    "# Conversión a arrays de Pennylane (diferenciables)\n",
    "X_train = np.array(X_train.values, requires_grad=False)\n",
    "\n",
    "# Conversión de etiquetas {0,1} a {-1,1} (necesario para el clasificador cuántico)\n",
    "Y_train = np.array(y_train.values * 2 - np.ones(len(y_train)), requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99686ab7",
   "metadata": {},
   "source": [
    "## **III.** Etapa del Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15271527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del dispositivo cuántico con 4 qubits\n",
    "num_qubits = 4\n",
    "num_layers = 4\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)         # Simulador de qubits\n",
    "\n",
    "# Etapa del Feature Map\n",
    "def statepreparation(x):\n",
    "    qml.BasisEmbedding(x, wires=range(num_qubits))      # Codifica los datos clásicos como estados base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e731961",
   "metadata": {},
   "source": [
    "## **IV.** Etapa del Ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6ff57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del circuito cuántico completo\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(weights, x):\n",
    "    statepreparation(x)                         # Aplica el feature map (codificación)\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(num_qubits))                        # Aplica cada capa del ansatz\n",
    "    return qml.expval(qml.PauliZ(0))            # Retorna la expectativa de PauliZ en el qubit 0\n",
    "\n",
    "# Función del clasificador variacional\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias           # Predicción = salida del circuito + sesgo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ffd89",
   "metadata": {},
   "source": [
    "## **V.** Etapa de funcion de Pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ded8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss += (l - p) ** 2           # Error cuadrático por muestra\n",
    "    return loss / len(labels)         # Promedio del error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a173e38",
   "metadata": {},
   "source": [
    "## **VI.** Etapa de evaluación de la Función de Costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b48549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]     # Clasifica todos los datos\n",
    "    return square_loss(Y, predictions)                                      # Calcula el costo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122bcde",
   "metadata": {},
   "source": [
    "## **VII.** Etapa de Análisis de Precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305b52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    correct = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:               # Cuenta como correcta si predicción y etiqueta coinciden\n",
    "            correct += 1\n",
    "    return correct / len(labels)            # Porcentaje de aciertos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1148fe",
   "metadata": {},
   "source": [
    "## **VIII.** Pesos iniciales definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06cc5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semilla para reproducibilidad\n",
    "np.random.seed(0)               \n",
    "\n",
    "# Inicialización aleatoria de pesos\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)  \n",
    "\n",
    "# Sesgo inicial\n",
    "bias_init = np.array(0.0, requires_grad=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7be64e",
   "metadata": {},
   "source": [
    "## **IX.** Pesos finales entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebee0d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.8240705 | Accuracy: 0.5255930\n",
      "Iter:     2 | Cost: 1.7694584 | Accuracy: 0.5255930\n",
      "Iter:     3 | Cost: 1.7476024 | Accuracy: 0.5255930\n",
      "Iter:     4 | Cost: 1.7372305 | Accuracy: 0.5255930\n",
      "Iter:     5 | Cost: 1.7315333 | Accuracy: 0.5255930\n",
      "Iter:     6 | Cost: 1.7193486 | Accuracy: 0.5255930\n",
      "Iter:     7 | Cost: 1.7160961 | Accuracy: 0.5255930\n",
      "Iter:     8 | Cost: 1.7108819 | Accuracy: 0.5255930\n",
      "Iter:     9 | Cost: 1.7070357 | Accuracy: 0.5255930\n",
      "Iter:    10 | Cost: 1.6918464 | Accuracy: 0.5255930\n",
      "Iter:    11 | Cost: 1.6772090 | Accuracy: 0.5255930\n",
      "Iter:    12 | Cost: 1.6093775 | Accuracy: 0.5255930\n",
      "Iter:    13 | Cost: 1.5710094 | Accuracy: 0.5255930\n",
      "Iter:    14 | Cost: 1.4893539 | Accuracy: 0.5255930\n",
      "Iter:    15 | Cost: 1.4072849 | Accuracy: 0.5255930\n",
      "Iter:    16 | Cost: 1.3249171 | Accuracy: 0.5255930\n",
      "Iter:    17 | Cost: 1.1882233 | Accuracy: 0.5255930\n",
      "Iter:    18 | Cost: 1.1125386 | Accuracy: 0.5255930\n",
      "Iter:    19 | Cost: 1.0375188 | Accuracy: 0.7240949\n",
      "Iter:    20 | Cost: 0.9738281 | Accuracy: 0.7240949\n",
      "Iter:    21 | Cost: 0.9207622 | Accuracy: 0.7240949\n",
      "Iter:    22 | Cost: 0.9067335 | Accuracy: 0.7240949\n",
      "Iter:    23 | Cost: 0.9016452 | Accuracy: 0.7240949\n",
      "Iter:    24 | Cost: 0.9026295 | Accuracy: 0.7240949\n",
      "Iter:    25 | Cost: 0.8795519 | Accuracy: 0.7240949\n"
     ]
    }
   ],
   "source": [
    "opt = GradientDescentOptimizer(stepsize=0.1)\n",
    "num_it = 25\n",
    "batch_size = math.floor(len(X_train) / num_it)\n",
    "\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "\n",
    "for it in range(num_it):\n",
    "    batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "    X_batch = X_train[batch_index]\n",
    "    Y_batch = Y_train[batch_index]\n",
    "    \n",
    "    weights, bias = opt.step(lambda w, b: cost(w, b, X_batch, Y_batch), weights, bias)\n",
    "\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X_train]\n",
    "    acc = accuracy(Y_train, predictions)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f}\".format(\n",
    "            it + 1, cost(weights, bias, X_train, Y_train), acc\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a6112c",
   "metadata": {},
   "source": [
    "## **X.** Evaluación de desempeño del clasificador cuantico final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4769855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6777777777777778\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.17142857142857143\n",
      "F1 Score (macro): 0.5420249166520442\n"
     ]
    }
   ],
   "source": [
    "# Evaluación de desempeño del clasificador\n",
    "X_test = np.array(X_test.values, requires_grad=False)\n",
    "Y_test = np.array(y_test.values * 2 - np.ones(len(y_test)), requires_grad=False)\n",
    "\n",
    "# Realiza las predicciones con el modelo de entrenado\n",
    "predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "# Se evalua el desempeño con las metricas de exactitud, precisión y recall\n",
    "print(\"Accuracy Score:\", accuracy_score(Y_test, predictions))\n",
    "print(\"Precision Score:\", precision_score(Y_test, predictions))\n",
    "print(\"Recall Score:\", recall_score(Y_test, predictions))\n",
    "print(\"F1 Score (macro):\", f1_score(Y_test, predictions, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e0e7f5",
   "metadata": {},
   "source": [
    "## Comparación de los dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342156ef",
   "metadata": {},
   "source": [
    "### Tabla de comparacion de los dos modelos\n",
    "\n",
    "| **Métrica**   | **Modelo Original** | **Modelo Modificado** | **Observación** |\n",
    "| ------------- | ------------------- | --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Accuracy**  | 78.89%              | 67.80%                | El modelo modificado clasifica correctamente una menor cantidad de ejemplos totales.                                                      |\n",
    "| **Precision** | 76.77%              | 100%                  | El modelo modificado nunca comete falsos positivos, pero esto puede deberse a que predice muy pocos positivos.                              |\n",
    "| **Recall**    | 65.71%              | 17.10%                | El modelo modificado identifica solo una pequeña parte de los verdaderos positivos, lo que sugiere una alta tasa de falsos negativos.  |\n",
    "| **F1 Score**  | 77.12%              | 54.20%                | El modelo modificado tiene un rendimiento general más bajo, debido al desbalance entre precisión y recall.                                     |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
